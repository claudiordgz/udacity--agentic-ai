{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a4fc9f67",
   "metadata": {},
   "source": [
    "Agent Evaluation\n",
    "\n",
    "This notebook demonstrates different agent evaluation approaches by creating test scenarios and evaluation criteria.\n",
    "\n",
    "## What we'll learn:\n",
    "\n",
    "- Metrics\n",
    "- Test Case\n",
    "- Evaluator\n",
    "- Approaches\n",
    "    - black box, \n",
    "    - single step, \n",
    "    - trajectory\n",
    "- Evaluation Report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c08fcd",
   "metadata": {},
   "source": [
    "## Setup\n",
    "First, let's import the necessary libraries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1136c582-e487-4066-ba36-26258b7c35da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "nb_dir = Path(__file__).parent if \"__file__\" in globals() else Path.cwd()\n",
    "parent = nb_dir.parent  # points to 03-building-agents\n",
    "if str(parent) not in sys.path:\n",
    "    sys.path.insert(0, str(parent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e3448288",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only needed for Udacity workspace\n",
    "\n",
    "import importlib.util\n",
    "import sys\n",
    "\n",
    "# Check if 'pysqlite3' is available before importing\n",
    "if importlib.util.find_spec(\"pysqlite3\") is not None:\n",
    "    import pysqlite3\n",
    "    sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f655eb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import List, Dict\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from lib.agents import Agent, AgentState\n",
    "from lib.state_machine import Run\n",
    "from lib.messages import AIMessage\n",
    "from lib.tooling import tool\n",
    "from lib.evaluation import TestCase, AgentEvaluator, EvaluationResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ad4bd02c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe324c7",
   "metadata": {},
   "source": [
    "## Your Custom Tools\n",
    "\n",
    "Design and implement tools that will showcase different aspects of agent behavior. Consider creating tools that:\n",
    "\n",
    "- Require multi-step reasoning\n",
    "- Handle different data types and formats\n",
    "- Interact with external services or APIs\n",
    "- Process complex inputs\n",
    "- Have potential failure modes to test error handling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14b6d8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List, Literal, Optional\n",
    "\n",
    "class ToolStatus(TypedDict, total=False):\n",
    "    status: Literal[\"ok\", \"no_context\", \"error\"]\n",
    "    error: Optional[str]\n",
    "\n",
    "class GameItem(TypedDict):\n",
    "    Game: str\n",
    "    Platform: str\n",
    "    Score: int\n",
    "\n",
    "class GetGamesResult(ToolStatus, total=False):\n",
    "    games: List[GameItem]\n",
    "\n",
    "@tool\n",
    "def get_games(num_games: int = 1, top: bool = True) -> GetGamesResult:\n",
    "    \"\"\"\n",
    "    Return top or bottom N games by score.\n",
    "\n",
    "    Inputs:\n",
    "      num_games (int): number of games to return (default 1)\n",
    "      top (bool): True for top, False for bottom\n",
    "\n",
    "    Returns schema:\n",
    "      {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"status\": {\"type\":\"string\",\"enum\":[\"ok\",\"no_context\",\"error\"]},\n",
    "          \"error\": {\"type\":\"string\"},\n",
    "          \"games\": {\n",
    "            \"type\":\"array\",\n",
    "            \"items\":{\"type\":\"object\",\"properties\":{\n",
    "              \"Game\":{\"type\":\"string\"},\n",
    "              \"Platform\":{\"type\":\"string\"},\n",
    "              \"Score\":{\"type\":\"integer\"}\n",
    "            }}\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    \"\"\"\n",
    "    data: List[GameItem] = [\n",
    "        {\"Game\": \"The Legend of Zelda: Breath of the Wild\", \"Platform\": \"Switch\", \"Score\": 98},\n",
    "        {\"Game\": \"Super Mario Odyssey\", \"Platform\": \"Switch\", \"Score\": 97},\n",
    "        {\"Game\": \"Metroid Prime\", \"Platform\": \"GameCube\", \"Score\": 97},\n",
    "        {\"Game\": \"Super Smash Bros. Brawl\", \"Platform\": \"Wii\", \"Score\": 93},\n",
    "        {\"Game\": \"Mario Kart 8 Deluxe\", \"Platform\": \"Switch\", \"Score\": 92},\n",
    "        {\"Game\": \"Fire Emblem: Awakening\", \"Platform\": \"3DS\", \"Score\": 92},\n",
    "        {\"Game\": \"Donkey Kong Country Returns\", \"Platform\": \"Wii\", \"Score\": 87},\n",
    "        {\"Game\": \"Luigi's Mansion 3\", \"Platform\": \"Switch\", \"Score\": 86},\n",
    "        {\"Game\": \"Pikmin 3\", \"Platform\": \"Wii U\", \"Score\": 85},\n",
    "        {\"Game\": \"Animal Crossing: New Leaf\", \"Platform\": \"3DS\", \"Score\": 88},\n",
    "    ]\n",
    "    if num_games <= 0:\n",
    "        return {\"status\": \"no_context\", \"games\": []}\n",
    "    sorted_games = sorted(data, key=lambda x: x[\"Score\"], reverse=top)\n",
    "    return {\"status\": \"ok\", \"games\": sorted_games[:num_games]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eb0b328",
   "metadata": {},
   "source": [
    "## Develop your Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf079568",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(\n",
    "    model_name=\"gpt-4o-mini\",\n",
    "    tools=[get_games],\n",
    "    instructions=\"You can bring insights about a game dataset based on users questions\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c62c60",
   "metadata": {},
   "source": [
    "## Design Your Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8abd8653",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = AgentEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11dd713c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = [\n",
    "    TestCase(\n",
    "        id=\"game_query_1\",\n",
    "        description=\"Find the best game in the dataset\",\n",
    "        user_query=\"What's the best game in the dataset?\",\n",
    "        expected_tools=[\"get_games\"],\n",
    "        reference_answer=\"The Legend of Zelda: Breath of the Wild with score 98\",\n",
    "        max_steps=4\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53118ad7",
   "metadata": {},
   "source": [
    "## Run Your Evaluation Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08152474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Evaluating Test Case: game_query_1 ===\n",
      "Description: Find the best game in the dataset\n",
      "Query: What's the best game in the dataset?\n",
      "\n",
      "Workflow:\n",
      "[StateMachine] Starting: __entry__\n",
      "[StateMachine] Executing step: message_prep\n",
      "[StateMachine] Executing step: llm_processor\n",
      "[StateMachine] Executing step: tool_executor\n",
      "[StateMachine] Executing step: llm_processor\n",
      "[StateMachine] Terminating: __termination__\n",
      "\n",
      "--- Black Box (Final Response) Evaluation ---\n",
      "Overall Score: 1.00\n",
      "Task Completed: True\n",
      "Feedback: The agent response successfully identifies the best game in the dataset, providing the name, platform, and score, which fully answers the user's query. The format is appropriate as it clearly presents the information. Additionally, the response follows the implicit instruction to provide a clear and concise answer.\n",
      "\n",
      "--- Single Step Evaluation ---\n",
      "Overall Score: 1.00\n",
      "Correct Tool Selected: True\n",
      "Feedback: Selected tools: ['get_games'], Expected: ['get_games']\n",
      "\n",
      "--- Trajectory Evaluation ---\n",
      "Overall Score: 1.00\n",
      "Steps Taken: 4\n",
      "Total Tokens: 713\n",
      "Execution Time: 3.96s\n",
      "Estimated Cost: $0.000267\n",
      "Feedback: Trajectory: 4 steps, Tools used: ['get_games'], Expected: ['get_games']\n"
     ]
    }
   ],
   "source": [
    "for test_case in test_cases:\n",
    "    print(f\"\\n=== Evaluating Test Case: {test_case.id} ===\")\n",
    "    print(f\"Description: {test_case.description}\")\n",
    "    print(f\"Query: {test_case.user_query}\")\n",
    "    \n",
    "    # Run the agent\n",
    "    start_time = time.time()\n",
    "    print(\"\\nWorkflow:\")\n",
    "    agent.memory.reset()\n",
    "    run_object:Run = agent.invoke(test_case.user_query)\n",
    "    execution_time = time.time() - start_time\n",
    "    \n",
    "    # Get final state and response\n",
    "    final_state:AgentState = run_object.get_final_state()\n",
    "    if final_state and final_state.get(\"messages\"):\n",
    "        # Find the last AI message as the final response\n",
    "        final_response = \"\"\n",
    "        for msg in reversed(final_state[\"messages\"]):\n",
    "            if isinstance(msg, AIMessage) and msg.content:\n",
    "                final_response = msg.content\n",
    "                break\n",
    "        \n",
    "        total_tokens = final_state.get(\"total_tokens\", 0)\n",
    "        \n",
    "        # Evaluate using all three methods\n",
    "        print(\"\\n--- Black Box (Final Response) Evaluation ---\")\n",
    "        black_box_eval:EvaluationResult = evaluator.evaluate_final_response(\n",
    "            test_case, final_response, execution_time, total_tokens\n",
    "        )\n",
    "        print(f\"Overall Score: {black_box_eval.overall_score:.2f}\")\n",
    "        print(f\"Task Completed: {black_box_eval.task_completion.task_completed}\")\n",
    "        print(f\"Feedback: {black_box_eval.feedback}\")\n",
    "        \n",
    "        print(\"\\n--- Single Step Evaluation ---\")\n",
    "        step_eval:EvaluationResult = evaluator.evaluate_single_step(\n",
    "            final_state[\"messages\"], test_case.expected_tools\n",
    "        )\n",
    "        print(f\"Overall Score: {step_eval.overall_score:.2f}\")\n",
    "        print(f\"Correct Tool Selected: {step_eval.tool_interaction.correct_tool_selected}\")\n",
    "        print(f\"Feedback: {step_eval.feedback}\")\n",
    "        \n",
    "        print(\"\\n--- Trajectory Evaluation ---\")\n",
    "        traj_eval:EvaluationResult = evaluator.evaluate_trajectory(test_case, run_object)\n",
    "        print(f\"Overall Score: {traj_eval.overall_score:.2f}\")\n",
    "        print(f\"Steps Taken: {traj_eval.task_completion.steps_taken}\")\n",
    "        print(f\"Total Tokens: {traj_eval.system_metrics.total_tokens}\")\n",
    "        print(f\"Execution Time: {traj_eval.system_metrics.execution_time:.2f}s\")\n",
    "        print(f\"Estimated Cost: ${traj_eval.system_metrics.cost_estimate:.6f}\")\n",
    "        print(f\"Feedback: {traj_eval.feedback}\")\n",
    "        \n",
    "    else:\n",
    "        print(\"ERROR: No final state or messages found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64fe7692",
   "metadata": {},
   "source": [
    "## Reflection and Iteration\n",
    "\n",
    "Based on your evaluation results:\n",
    "\n",
    "1. **Identify Patterns**: What trends do you see in your agent's performance?\n",
    "2. **Spot Weaknesses**: Where does your agent struggle most?\n",
    "3. **Recognize Strengths**: What does your agent do particularly well?\n",
    "4. **Iterate**: Modify your tools, test cases, or agent configuration and re-evaluate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6acdca-bbfe-4431-ac60-f3f13b9260fe",
   "metadata": {},
   "source": [
    "# Patterns\n",
    "  \n",
    "  - Consistent tool selection when the query maps clearly to one tool.\n",
    "  - Short trajectories (≈4 steps) and reasonable token use (hundreds per run).\n",
    "  - Final responses concise and on-target for well-scoped prompts.\n",
    "\n",
    "# Weaknesses\n",
    "\n",
    "  - Limited coverage: few test cases means blind spots (ambiguous queries, missing params, multi-step reasoning).\n",
    "  - Fallback behavior only triggers when tools return “no_context”; needs broader tests (conflicting tools, empty results, errors).\n",
    "  - Little stress on token budget (many tools, long prompts, long chats) in evaluations.\n",
    "\n",
    "# Strengths\n",
    "\n",
    "  - Unified tool return envelope (status/no_context/error) works; agent can reason on structured outputs.\n",
    "  - Auto-included tool docs and return schemas reduce prompt handcrafting.\n",
    "  - Token measurement and budget pruning integrated; observable total tokens and costs.\n",
    "\n",
    "# Iterate (next actions)\n",
    "\n",
    "## Expand test coverage:\n",
    "\n",
    "  - Ambiguous queries (should ask clarifying Qs before tool use).\n",
    "  - Missing parameters (ensure “ask before call” behavior).\n",
    "  - Multi-tool flows (e.g., search → compare → answer).\n",
    "  - Error paths (tool errors/timeouts/empty results).\n",
    "\n",
    "## Strengthen fallback/routing:\n",
    "\n",
    "  - Add tests where first tool returns “ok” but low-signal answer; require trying another tool.\n",
    "  - Add a simple “confidence” heuristic (e.g., answer length/keywords) to gate fallback.\n",
    "\n",
    "## Tighten token discipline:\n",
    "\n",
    "  - Add tests with many tools/long history; verify pruning and summaries kick in.\n",
    "  - Log per-step tokens and cumulative_tokens in trajectory metrics.\n",
    "\n",
    "## Dial reliability:\n",
    "\n",
    "  - Lower temperature for evaluations (e.g., 0.1–0.2) for consistency.\n",
    "  - Add one-shot examples in instructions for tricky patterns (only if not fine-tuning).\n",
    "\n",
    "## Data realism:\n",
    "\n",
    "  - Add noisy/edge “game” entries and verify sorting/formatting.\n",
    "  - Add adversarial prompts to validate tool doc adherence and injection resistance.\n",
    "\n",
    "## Optional enhancements\n",
    "\n",
    "  - Include tool latency and success/failure rates in evaluation output.\n",
    "  - Add a “return format contract” test (JSON shape validation against return schema)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fe2754-eded-4978-8f7b-c7f083443864",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
